# Forex-Forecasting-by-simple-DNNs
Machine learning algorithms have been used to predict future OHLC prices. The previous findings suggest that machine learning algorithms can predict future OHLC prices properly and affect price forecasts. We utilized FLF and Mean Square Error as loss functions, and developed a deep neural network model using Convolutional Neural Net-
works by incorporating multiple time frames into the training process to reduce loss value between actual and predicted prices in the compression with simple GRU, LSTM, and CNN models. This repository contains implementations of deep neural networks based on each data sequence of size 70 to predict 71st. During my research, I compared these models to my suggested model.

# Long-short Term Memory (LSTM)
The total input to an LSTM [12] unit is comprised of the current input from the previous layer and the recurrent input from the previous time step. LSTM units are comprised of three different parts of forget, input, and output gates. The forget gate is responsible for keeping or removing currently existing information on the cell state memory according the total input. Then the input gate extract new information from the total input and add the important information that is most probably required
in next time steps to the cell-state memory. Finally, the output gate computes the output from the cell-state memory based on the total input and send to the next layer and to the LSTM unit itself via the recurrent connection.

# Gated Recurrent Unit (GRU)
Compared to LSTM, GRU has a more straightforward structure and simpler algorithm. LSTM has three gates, while GRU has only two. GRUs also have input gates and forget gates but have fewer parameters than LSTMs due to their lack of an output gate. The forget gate and the input gate, and the cell state and the hidden state are combined in a single gate named the update gate and the rest gate, respectively. The update gate is responsible for transferring information from the past step to the future. The rest gate is responsible for long-short term memory, which keeps or removes information of total inputs.

# Convolutional Neural Network
Convolutional neural networks CNNs are usually used to solve 2d and 3d visual tasks such as image classification, object detection, image segmentation, and etc. One-dimensional convolutions can be applied to sequential data and time series. CNNs are made of a cascade of interlaying convolutional and pooling layers followed by a number of fully-connected layers. Convolutonal layers detect informative local features by applying multiple filter kernels over the input signal. Then, pooling layers remove local redundancies through a subsampling operation (usually using a maximum kernel) and improve the robustness of the network to the variations in data. Through the layers, the feature complexities increase in such a way that higher convolutional layers detect discriminative patterns from the data that are well suited for the final classification or regression in fully-connected layers. 
